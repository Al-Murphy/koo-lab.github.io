# Papers
<br>

##########################

MAKE IN GOOGLE DOC IN KOO LAB SHARED

##########################

_____________________________________________________________
## Reviews
_____________________________________________________________

# Deep learning in genomics

### Reviews
* _Deep learning: new computational modelling techniques for genomics_ ([Link](https://www.nature.com/articles/s41576-019-0122-6)) 
* _Deep learning for inferring transcription factor binding sites_ 
([Link](https://www.sciencedirect.com/science/article/pii/S2452310020300032?via%3Dihub)) 

### Classic DL in genomics
* _Predicting the sequence specificities of DNA- and RNA-binding proteins by deep learning_ ([Link](https://www.nature.com/articles/nbt.3300))
* _Predicting effects of noncoding variants with deep learning–based sequence model_ ([Link](https://www.nature.com/articles/nmeth.3547))
* _Basset: Learning the regulatory code of the accessible genome with deep convolutional neural networks_ ([Link](https://genome.cshlp.org/content/early/2016/05/03/gr.200535.115.abstract))

### TF binding
* _Sequential regulatory activity prediction acrosschromosomes with convolutional neural networks_ ([Link](https://genome.cshlp.org/content/early/2018/04/09/gr.227819.117.full.pdf))
* _Deep learning at base-resolution reveals cis-regulatory motif syntax_ ([Link](https://www.biorxiv.org/content/10.1101/737981v2))


### RNA modeling

* _Predicting Splicing from Primary Sequence with Deep Learning_ ([Link](https://www.cell.com/cell/pdf/S0092-8674(18)31629-5.pdf))
* _A Deep Neural Network for Predicting and Engineering Alternative Polyadenylation_ ([Link](https://www.cell.com/cell/fulltext/S0092-8674(19)30498-2))
* _Human 5′ UTR design and variant effect prediction from a massively parallel translation assay_ ([Link](https://www.nature.com/articles/s41587-019-0164-5))




#### Attention-based models in genomics

https://academic.oup.com/bib/advance-article-abstract/doi/10.1093/bib/bbaa159/5890498?redirectedFrom=fulltext



#### Interpretability/Explainability

_____________________________________________________________

## Advanced Topics
_____________________________________________________________


#### Adversarial training
Adversarial training tutorial
https://adversarial-ml-tutorial.org/

#### Protein modeling



____________________________________________________________
### Deep Learning for Genomics

* _Enhanced Integrated Gradients: improving interpretability of deep learning models using splicing codes as a case study_ ([Link](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02055-7))


* _An equivariant Bayesian convolutional network predicts recombination hotspots and accurately resolves binding motifs_ ([Link](https://academic.oup.com/bioinformatics/article/35/13/2177/5210873))
* _Predicting Gene Expression from DNA Sequenceusing Residual Neural Network_ ([Link](https://www.biorxiv.org/content/10.1101/2020.06.21.163956v1.full.pdf))
* _Deep learning of the regulatory grammar of yeast 5′ untranslated regions from 500,000 random sequences_ ([Link](https://genome.cshlp.org/content/early/2017/11/02/gr.224964.117.abstract))

_____________________________________________________________
### TF Binding

* _Deciphering eukaryotic gene-regulatory logic with 100 million random promotesr_ ([Link](https://www.nature.com/articles/s41587-019-0315-8))
* _Intrinsically Disordered Regions Direct Transcription Factor In Vivo Binding Specificity_ ([Link](https://www.cell.com/molecular-cell/fulltext/S1097-2765(20)30352-X?rss=yes))
* _Virtual ChIP-seq: predicting transcription factor binding by learning from the transcriptome_ ([Link](https://www.biorxiv.org/content/10.1101/168419v4.full.pdf))
* _FactorNet: A deep learning framework for predicting cell type specific transcription factor binding from nucleotide-resolution sequential data_ ([Link](https://www.sciencedirect.com/science/article/pii/S1046202318303293?via%3Dihub))
* _Completing the ENCODE3 compendium yields accurate imputations across a variety of assays and human biosamples_ ([Link](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-01978-5#Sec9))
* _Multi-scale deep tensor factorization learns a latentrepresentation of the human epigenome_ ([Link](https://www.biorxiv.org/content/biorxiv/early/2019/04/11/364976.full.pdf))
* _Single molecule occupancy patterns of transcription factors reveal determinants of cooperative binding in vivo_ ([Link](https://www.biorxiv.org/content/10.1101/2020.06.29.167155v1))
* _Single molecule occupancy patterns oftranscription factors reveal determinants ofcooperative binding in vivo_ ([Link](https://www.biorxiv.org/content/10.1101/2020.06.29.167155v1.full.pdf))
* _An interpretable bimodal neural network characterizes the sequence and preexisting chromatin predictors of induced TF binding_ ([Link](https://www.biorxiv.org/content/10.1101/672790v2))

_____________________________________________________________
### RBP Binding

* _Predicting dynamic cellular protein-RNA interactions using deep learning and in vivo RNA structure_ ([Link](https://www.biorxiv.org/content/biorxiv/early/2020/05/07/2020.05.05.078774.full.pdf))
* _Epigenome-based splicing prediction using a recurrent neural network_ ([Link](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008006))

_____________________________________________________________
### Higher-order interactions

* _Improved Models for Transcription Factor Binding Site Identification Using Nonindependent Interactions_ ([Link](https://www.genetics.org/content/191/3/781.long))
* _A unified approach for quantifying and interpreting DNA shape readout by transcription factors_ ([Link](https://www.embopress.org/doi/full/10.15252/msb.20177902))
* _Learning from mistakes: Accurate prediction of cell type-specific2transcription factor binding_ ([Link](https://www.biorxiv.org/content/biorxiv/early/2018/06/12/230011.full.pdf))
* _Varying levels of complexity in transcription factor binding motifs_ ([Link](https://academic.oup.com/nar/article/43/18/e119/2414334))
* _A General Pairwise Interaction Model Provides an Accurate Description of In Vivo Transcription Factor Binding Sites_ ([Link](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0099015))


_____________________________________________________________
### Proteins

* _Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences_ ([Link](https://www.biorxiv.org/content/10.1101/622803v2))
* _Evaluating Protein Transfer Learning with TAPE_ ([Link](https://papers.nips.cc/paper/9163-evaluating-protein-transfer-learning-with-tape.pdf))
* _Using Deep Learning to Annotate the Protein Universe_ ([Link](https://www.biorxiv.org/content/10.1101/626507v4))
* _Improved protein structure prediction using predicted interresidue orientations_ ([Link](https://www.pnas.org/content/117/3/1496
))
* _Unified rational protein engineering with sequence-based deep representation learning_ ([Link](https://www.nature.com/articles/s41592-019-0598-1.pdf?proof=true))
* _Deep generative models of genetic variation capture the effects of mutations_ ([Link](https://www.nature.com/articles/s41592-018-0138-4))
* _Learning protein sequence embeddings using information from structure_ ([Link](https://arxiv.org/pdf/1902.08661.pdf))
* _Transforming the language of life: Transformer netowrks for protein prediction tasks_ ([Link](https://www.biorxiv.org/content/10.1101/2020.06.15.153643v1.full.pdf))
)
* _The proteome landscape of the kingdoms of life_ ([Link](https://www.nature.com/articles/s41586-020-2402-x)
* _Energy-based models for atomic resolution protein conformations_ ([Link](https://openreview.net/pdf?id=S1e_9xrFvS#page5))

* _BERTology Meets Biology: Interpreting Attentionin Protein Language Models_ ([Link](https://www.biorxiv.org/content/10.1101/2020.06.26.174417v1.full.pdf))
* _A Generative Neural Network for Maximizing Fitness and Diversity of Synthetic DNA and Protein Sequences_ ([Link](https://www.cell.com/cell-systems/fulltext/S2405-4712(20)30192-7))

_____________________________________________________________
### Attention/Transformers

* _The utility of Einsum_ ([Link](https://rockt.github.io/2018/04/30/einsum))
* _The Transformer Family_ ([Link](https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html))
* _Two approaches to faster attention_ ([Link](https://yangkky.github.io/2020/06/22/attention.html))
* _SYNTHESIZER: Rethinking Self-Attention in Transformer Models_ ([Link](https://arxiv.org/pdf/2005.00743.pdf))
* _Linformer: Self-Attention with Linear Complexity_ ([Link](https://arxiv.org/abs/2006.04768))
* _Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers_ ([Link](https://arxiv.org/abs/2006.03555))
* _Rethinking positional encoding in language pre-training_ ([Link](https://arxiv.org/pdf/2006.15595.pdf))
* _Self-Attention with Relative Position Representations_ ([Link](https://arxiv.org/pdf/1803.02155.pdf))

_____________________________________________________________
### Deep Learning

* _EfficientDet: Scalable and Efficient Object Detection_ ([Link](https://arxiv.org/pdf/1911.09070.pdf))
* _Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One_ ([Link](https://arxiv.org/abs/1912.03263))
* _Gradient Estimation with Stochastic Softmax Tricks_ ([Link](https://arxiv.org/abs/2006.08063))
* _Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning_ ([Link](https://arxiv.org/abs/2006.07733))
* _PatchUp: A Regularization Technique for Convolutional Neural Networks_ ([Link](https://arxiv.org/abs/2006.07794))
* _Domain Extrapolation via Regret Minimization_ ([Link](https://arxiv.org/abs/2006.03908))
* _Denoising Diffusion Probabilistic Models_ ([Link](https://hojonathanho.github.io/diffusion/assets/denoising_diffusion20.pdf))
* _Understanding deep learning requires rethinking generalization_ ([Link](https://arxiv.org/abs/1611.03530))


_____________________________________________________________
### Interpretability

* _Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps_ ([Link](https://arxiv.org/abs/1312.6034))
* _Axiomatic Attribution for Deep Networks_ ([Link](https://arxiv.org/pdf/1703.01365.pdf))
* _SmoothGrad: removing noise by adding noise_ ([Link](https://arxiv.org/abs/1706.03825))
* _“Why Should I Trust You?”Explaining the Predictions of Any Classifier_ ([Link](https://arxiv.org/pdf/1602.04938v1.pdf))
* _Explaining Models by Propagating Shapley Values_ ([Link](https://arxiv.org/pdf/1911.11888.pdf))
* _A Unified Approach to Interpreting Model Predictions_ ([Link](https://arxiv.org/abs/1705.07874))
* _Learning Important Features Through Propagating Activation Differences_ ([Link](https://arxiv.org/abs/1704.02685))
* _Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)_ ([Link](https://arxiv.org/abs/1711.11279))
* _Learning Explainable Models Using Attribution Priors_ ([Link](https://arxiv.org/abs/1906.10670))
* _How does this interaction affect me? Interpretable attribution for feature interactions_ ([Link](https://arxiv.org/pdf/2006.10965.pdf))
* _Explaining Explanations: Axiomatic Feature Interactions for Deep Networks_ ([Link](https://arxiv.org/abs/2002.04138))
* _Understanding and misunderstanding randomized controlled trials_ ([Link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6019115/pdf/nihms930764.pdf))

_____________________________________________________________
### Robustness 

* _Gradient Science Blog -- Madry Lab at MIT_ ([Link](https://gradientscience.org/))
* _Adversarial Training Tutorial_ ([Link](https://adversarial-ml-tutorial.org/))
* _Feature Purification: How Adversarial Training Performs Robust Deep Learning_ ([Link](https://www.microsoft.com/en-us/research/uploads/prod/2020/05/2005.10190_Feature-Purification_How-Adversarial-Training-Performs-Robust-Deep-Learning.pdf))
* _How benign is benign overfitting?_ ([Link](https://arxiv.org/abs/2007.04028))
* _Intriguing properties of neural networks_ ([Link](https://arxiv.org/abs/1312.6199))
* _On the Robustness of Interpretability Methods_ ([Link](https://arxiv.org/pdf/1806.08049.pdf))
* _Robustness May Be at Odds with Accuracy_ ([Link](https://arxiv.org/abs/1805.12152))
* _Adversarial Examples are not Bugs, they are Features_ ([Link](https://papers.nips.cc/paper/8307-adversarial-examples-are-not-bugs-they-are-features.pdf))
* _Towards Deep Learning Models Resistant to Adversarial Attacks_ ([Link](https://openreview.net/pdf?id=rJzIBfZAb))
* _Certified Adversarial Robustness via Randomized Smoothing_ ([Link](https://arxiv.org/pdf/1902.02918.pdf))
* _Black-box Smoothing: A Provable Defense for Pretrained Classifiers_ ([Link](https://arxiv.org/pdf/2003.01908.pdf))
* _Adversarial Robustness Through Local Lipschitzness_ ([Link](https://arxiv.org/abs/2003.02460))
* _Smooth Adversarial Training_ ([Link](https://arxiv.org/abs/2006.14536))
* _Second-Order Provable Defenses against Adversarial Attacks_ ([Link](https://arxiv.org/abs/2006.00731))

_____________________________________________________________
### Variational Inference/Normalizing Flows

* _NVAE: A Deep Hierarchical Variational Autoencoder_ ([Link](https://arxiv.org/abs/2007.03898))
* _Variational Inference: A Review for Statisticians - Blei et al._ ([Link](https://arxiv.org/pdf/1601.00670.pdf))
* _Auto-Encoding Variational Bayes_ ([Link](https://arxiv.org/abs/1312.6114))
* _Stochastic Backpropagation and Approximate Inference in Deep Generative Models_ ([Link](https://arxiv.org/abs/1401.4082))
* _Variational Inference with Normalizing Flows_ ([Link](https://arxiv.org/pdf/1505.05770.pdf))
* _Normalizing Flows: An Introduction and Review of Current Methods_ ([Link](https://arxiv.org/pdf/1908.09257.pdf))
* _Why Normalizing Flows Fail to Detect Out-of-Distribution Data_ ([Link](https://arxiv.org/abs/2006.08545))



_____________________________________________________________
### Dimensionality Reduction

* _How to Use t-SNE Effectively_ ([Link](https://distill.pub/2016/misread-tsne/))
* _How Exactly UMAP Works_ ([Link](https://towardsdatascience.com/how-exactly-umap-works-13e3040e1668))
* _Principal Component Analysis Breakdown_ ([Link](https://towardsdatascience.com/principal-component-analysis-breakdown-f3fb1fb48efc))


_____________________________________________________________
### Other

* _Amortized Causal Discovery: Learning to Infer Causal Graphs from Time-Series Data_ ([Link](https://arxiv.org/abs/2006.10833))
* _Discovering Symbolic Models from Deep Learningwith Inductive Biases_ ([Link](https://arxiv.org/pdf/2006.11287.pdf))
* _Reproducing kernel Hilbert spaces in Machine Learning - Lecture notes_ ([Link](http://www.gatsby.ucl.ac.uk/~gretton/coursefiles/rkhscourse.html))
* _From development to deployment: dataset shift, causality, and shift-stable models in health AI_ ([Link](https://academic.oup.com/biostatistics/article/21/2/345/5631850))
* _Fair Generative Modeling via Weak Supervision_ ([Link](https://arxiv.org/abs/1910.12008))
* _Learning compositional functions via multiplicative weight updates_ ([Link](https://arxiv.org/abs/2006.14560))


<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
